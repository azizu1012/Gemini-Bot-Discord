
import asyncio
import json
import re
import os
import random
from datetime import datetime, timedelta
import aiofiles
import requests
import sympy as sp
from google.generativeai.types import Tool, FunctionDeclaration
from serpapi import GoogleSearch
from tavily import TavilyClient
import exa_py
from typing import Any, Dict, Tuple, Optional
from config import (
    logger,
    NOTE_PATH,
    WEATHER_API_KEY,
    CITY,
    WEATHER_CACHE_PATH,
    SERPAPI_API_KEY,
    TAVILY_API_KEY,
    EXA_API_KEY,
    GOOGLE_CSE_ID,
    GOOGLE_CSE_API_KEY,
    GOOGLE_CSE_ID_1,
    GOOGLE_CSE_API_KEY_1,
    GOOGLE_CSE_ID_2,
    GOOGLE_CSE_API_KEY_2
)

# --- ƒê·ªäNH NGHƒ®A TOOLS CHO GEMINI ---
ALL_TOOLS = [
    Tool(function_declarations=[
        FunctionDeclaration(
            name="web_search",
            description=(
                "T√¨m ki·∫øm th√¥ng tin c·∫≠p nh·∫≠t (tin t·ª©c, gi√° c·∫£, phi√™n b·∫£n game, s·ª± ki·ªán) sau nƒÉm 2024. "
                "Ch·ªâ d√πng khi ki·∫øn th·ª©c n·ªôi b·ªô c·ªßa b·∫°n ƒë√£ l·ªói th·ªùi so v·ªõi ng√†y hi·ªán t·∫°i. "
                "Y√™u c·∫ßu T·ª∞ D·ªäCH c√¢u h·ªèi ti·∫øng Vi·ªát c·ªßa user th√†nh m·ªôt query t√¨m ki·∫øm ti·∫øng Anh T·ªêI ∆ØU."
            ),
            parameters={
                "type": "object",
                "properties": {"query": {"type": "string", "description": "C√¢u h·ªèi b·∫±ng ti·∫øng Anh"}},
                "required": ["query"]
            }
        )
    ]),
    Tool(function_declarations=[
        FunctionDeclaration(
            name="get_weather",
            description="L·∫•y th√¥ng tin th·ªùi ti·∫øt hi·ªán t·∫°i cho m·ªôt th√†nh ph·ªë c·ª• th·ªÉ.",
            parameters={
                "type": "object",
                "properties": {"city": {"type": "string", "description": "T√™n th√†nh ph·ªë, v√≠ d·ª•: 'Hanoi', 'Tokyo'."}},
                "required": ["city"]
            }
        )
    ]),
    Tool(function_declarations=[
        FunctionDeclaration(
            name="calculate",
            description="Gi·∫£i c√°c b√†i to√°n s·ªë h·ªçc ho·∫∑c bi·ªÉu th·ª©c ph·ª©c t·∫°p, bao g·ªìm c√°c h√†m l∆∞·ª£ng gi√°c, logarit, v√† ƒë·∫°i s·ªë.",
            parameters={
                "type": "object",
                "properties": {"equation": {"type": "string", "description": "Bi·ªÉu th·ª©c to√°n h·ªçc d∆∞·ªõi d·∫°ng string, v√≠ d·ª•: 'sin(pi/2) + 2*x'."}},
                "required": ["equation"]
            }
        )
    ]),
    Tool(function_declarations=[
        FunctionDeclaration(
            name="save_note",
            description="L∆∞u m·ªôt m·∫©u th√¥ng tin, ghi ch√∫ ho·∫∑c l·ªùi nh·∫Øc c·ª• th·ªÉ theo y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ b·∫°n c√≥ th·ªÉ truy c·∫≠p l·∫°i sau.",
            parameters={
                "type": "object",
                "properties": {"note": {"type": "string", "description": "N·ªôi dung ghi ch√∫ c·∫ßn l∆∞u."}},
                "required": ["note"]
            }
        )
    ]),
]

# === B·ªò ƒêI·ªÄU PH·ªêI TOOL ===
async def call_tool(function_call: Any, user_id: str) -> str:
    name = function_call.name
    args = dict(function_call.args)
    logger.info(f"TOOL G·ªåI: {name} | Args: {args} | User: {user_id}")

    try:
        if name == "web_search":
            query = args.get("query", "")
            return await run_search_apis(query, "general")

        elif name == "get_weather":
            city = args.get("city", "Ho Chi Minh City")
            data = await get_weather(city)
            return json.dumps(data, ensure_ascii=False, indent=2)

        elif name == "calculate":
            eq = args.get("equation", "")
            return await asyncio.to_thread(run_calculator, eq)

        elif name == "save_note":
            note = args.get("note", "")
            return await save_note(note)

        else:
            return "Tool kh√¥ng t·ªìn t·∫°i!"

    except Exception as e:
        logger.error(f"Tool {name} l·ªói: {e}")
        return f"L·ªói tool: {str(e)}"

# --- B·∫¢N ƒê·ªí T√äN TH√ÄNH PH·ªê ---
CITY_NAME_MAP = {
    "h·ªì ch√≠ minh": ("Ho Chi Minh City", "Th√†nh ph·ªë H·ªì Ch√≠ Minh"),
    "tp.hcm": ("Ho Chi Minh City", "Th√†nh ph·ªë H·ªì Ch√≠ Minh"),
    "s√†i g√≤n": ("Ho Chi Minh City", "Th√†nh ph·ªë H·ªì Ch√≠ Minh"),
    "ho chi minh city": ("Ho Chi Minh City", "Th√†nh ph·ªë H·ªì Ch√≠ Minh"),
    "hcmc": ("Ho Chi Minh City", "Th√†nh ph·ªë H·ªì Ch√≠ Minh"),
    "h√† n·ªôi": ("Hanoi", "H√† N·ªôi"),
    "ha noi": ("Hanoi", "H√† N·ªôi"),
    "danang": ("Da Nang", "ƒê√† N·∫µng"),
    "ƒë√† n·∫µng": ("Da Nang", "ƒê√† N·∫µng"),
    "da nang": ("Da Nang", "ƒê√† N·∫µng"),
}

def normalize_city_name(city_query: str) -> Tuple[str, str]:
    if not city_query:
        return ("Ho Chi Minh City", "Th√†nh ph·ªë H·ªì Ch√≠ Minh")
    city_key = city_query.strip().lower()
    for k, v in CITY_NAME_MAP.items():
        if k in city_key:
            return v
    return (city_query, city_query.title())

weather_lock = asyncio.Lock()

async def get_weather(city_query: Optional[str] = None) -> Dict[str, Any]:
    async with weather_lock:
        if city_query is None:
            city_query = CITY or "Ho Chi Minh City"
        city_en, city_vi = normalize_city_name(city_query)

        cache_path = WEATHER_CACHE_PATH.replace(".json", f"_{city_en.replace(' ', '_').lower()}.json")

        if await asyncio.to_thread(os.path.exists, cache_path):
            try:
                async with aiofiles.open(cache_path, 'r') as f:
                    cache = json.loads(await f.read())
                cache_time = datetime.fromisoformat(cache['timestamp'])
                if datetime.now() - cache_time < timedelta(hours=1):
                    return {**cache['data'], "city_vi": city_vi}
            except:
                pass

        if not WEATHER_API_KEY:
            default_data = {
                'current': f'M∆∞a r√†o s√°ng, m√¢y chi·ªÅu ·ªü {city_vi} (23-28¬∞C).',
                'forecast': [f'Ng√†y mai: N·∫Øng, 26¬∞C', f'Ng√†y kia: M∆∞a, 25¬∞C'] * 3,
                'timestamp': datetime.now().isoformat(),
                'city_vi': city_vi
            }
            with open(cache_path, 'w') as f:
                json.dump({'data': default_data, 'timestamp': datetime.now().isoformat()}, f)
            return default_data

        try:
            url = f"http://api.weatherapi.com/v1/forecast.json?key={WEATHER_API_KEY}&q={city_en}&days=7&aqi=no&alerts=no"
            response = requests.get(url, timeout=10)
            if response.status_code != 200:
                raise ValueError(f"API status: {response.status_code}")

            data = response.json()
            if 'error' in data:
                raise ValueError(f"API error: {data['error']['message']}")

            current = data['current']['condition']['text'] + f" ({data['current']['temp_c']}¬∞C)"
            forecast = []
            for day in data['forecast']['forecastday'][1:7]:
                forecast.append(f"Ng√†y {day['date']}: {day['day']['condition']['text']} ({day['day']['avgtemp_c']}¬∞C)")

            weather_data = {
                'current': current,
                'forecast': forecast,
                'timestamp': datetime.now().isoformat(),
                'city_vi': city_vi
            }

            cache_entry = {'data': weather_data, 'timestamp': datetime.now().isoformat()}
            with open(cache_path, 'w') as f:
                json.dump(cache_entry, f, indent=2)

            return weather_data
        except Exception as e:
            logger.error(f"Weather API l·ªói: {e}")
            fallback_data = {
                'current': f'L·ªói API, d√πng m·∫∑c ƒë·ªãnh: M∆∞a r√†o ·ªü {city_vi}, 23-28¬∞C.',
                'forecast': [f'Ng√†y mai: N·∫Øng, 26¬∞C', f'Ng√†y kia: M∆∞a, 25¬∞C'] * 3,
                'timestamp': datetime.now().isoformat(),
                'city_vi': city_vi
            }
            with open(cache_path, 'w') as f:
                json.dump({'data': fallback_data, 'timestamp': datetime.now().isoformat()}, f)
            return fallback_data

def run_calculator(equation_str: str) -> str:
    cleaned_eq = equation_str.strip().lower().replace('x', '*').replace(',', '.')
    if cleaned_eq.endswith('='):
        cleaned_eq = cleaned_eq[:-1]

    try:
        expr = sp.sympify(cleaned_eq, evaluate=False)
        result = sp.N(expr)
        result_str = str(result)
        if result_str.endswith('.0'):
            result_str = result_str[:-2]
        return json.dumps({
            "equation": equation_str,
            "result": result_str,
            "success": True
        }, ensure_ascii=False)
    except (sp.SympifyError, TypeError, ZeroDivisionError, Exception) as e:
        return json.dumps({
            "equation": equation_str,
            "result": f"L·ªói bi·ªÉu th·ª©c: {str(e)}",
            "success": False
        }, ensure_ascii=False)

async def save_note(query: str) -> str:
    try:
        note = query.lower().replace("ghi note: ", "").replace("save note: ", "").strip()
        async with aiofiles.open(NOTE_PATH, 'a', encoding='utf-8') as f:
            await f.write(f"[{datetime.now().isoformat()}] {note}\n")
        return f"ƒê√£ ghi note: {note}"
    except PermissionError:
        return "L·ªói: Kh√¥ng c√≥ quy·ªÅn ghi file notes.txt!"
    except Exception as e:
        return f"L·ªói ghi note: {str(e)}"

async def read_note() -> str:
    try:
        if not os.path.exists(NOTE_PATH):
            return "Ch∆∞a c√≥ note n√†o bro! Ghi note ƒëi nha! üòé"
        async with aiofiles.open(NOTE_PATH, 'r', encoding='utf-8') as f:
            notes = await f.readlines()
        if not notes:
            return "Ch∆∞a c√≥ note n√†o bro! Ghi note ƒëi nha! üòé"
        return "Danh s√°ch note:\n" + "".join(notes[-5:])
    except PermissionError:
        return "L·ªói: Kh√¥ng c√≥ quy·ªÅn ƒë·ªçc file notes.txt!"
    except Exception as e:
        return f"L·ªói ƒë·ªçc note: {str(e)}"

SEARCH_API_COUNTER = 0
SEARCH_LOCK = asyncio.Lock()
SEARCH_CACHE = {}
CACHE_LOCK = asyncio.Lock()

SEARCH_TOPICS = {
    # --- Core Topics (1-6) ---
    "gaming": {
        "keywords": ['game', 'patch', 'banner', 'update', 'release date', 'roadmap', 'leak', 'speculation', 'gacha', 'reroll', 'tier list', 'build', 'nh√¢n v·∫≠t', 'honkai', 'hsr', 'star rail', 'genshin', 'zzz', 'zenless', 'wuwa', 'wuthering waves', 'arknights', 'fgo', 'phi√™n b·∫£n', 's·ª± ki·ªán'],
        "suffixes": ["update", "release date", "patch notes", "roadmap", "leaks", "speculation", "official", "tin t·ª©c"]
    },
    "tech": {
        "keywords": ['tech', 'c√¥ng ngh·ªá', 'ai', 'ios', 'android', 'app', 'software', 'hardware', 'card m√†n h√¨nh', 'cpu', 'laptop', 'phone'],
        "suffixes": ["review", "release date", "news", "vs", "benchmark", "specs", "ƒë√°nh gi√°", "tin t·ª©c"]
    },
    "science": {
        "keywords": ['science', 'khoa h·ªçc', 'space', 'v≈© tr·ª•', 'nasa', 'discovery', 'research', 'nghi√™n c·ª©u', 'y t·∫ø'],
        "suffixes": ["new discovery", "latest research", "breakthrough", "study finds", "c√¥ng b·ªë", "nghi√™n c·ª©u m·ªõi"]
    },
    "finance": {
        "keywords": ['finance', 't√†i ch√≠nh', 'stock', 'c·ªï phi·∫øu', 'market', 'th·ªã tr∆∞·ªùng', 'investment', 'ƒë·∫ßu t∆∞', 'economy', 'kinh t·∫ø', 'l√£i su·∫•t', 'ng√¢n h√†ng'],
        "suffixes": ["stock price", "market analysis", "forecast", "news", "earnings report", "ph√¢n t√≠ch", "d·ª± b√°o"]
    },
    "movies_tv": {
        "keywords": ['movie', 'phim', 'tv show', 'series', 'netflix', 'disney+', 'trailer', 'actor', 'di·ªÖn vi√™n', 'ƒë·∫°o di·ªÖn', 'l·ªãch chi·∫øu'],
        "suffixes": ["review", "release date", "trailer", "cast", "ending explained", "season 2", "l·ªãch chi·∫øu phim", "ƒë√°nh gi√°"]
    },
    "anime_manga": {
        "keywords": ['anime', 'manga', 'light novel', 'manhwa', 'manhua', 'chapter', 'episode', 'season', 'ova', 'ph·∫ßn m·ªõi', 't·∫≠p m·ªõi'],
        "suffixes": ["release date", "new season", "chapter review", "discussion", "spoiler", "tin t·ª©c anime"]
    },
    # --- Entertainment & Hobbies (7-13) ---
    "sports": {
        "keywords": ['sports', 'th·ªÉ thao', 'b√≥ng ƒë√°', 'football', 'basketball', 'tennis', 'c·∫ßu l√¥ng', 'f1', 'ƒë·ªôi tuy·ªÉn', 'c·∫ßu th·ªß', 'tr·∫≠n ƒë·∫•u'],
        "suffixes": ["match result", "highlights", "live score", "news", "transfer", "l·ªãch thi ƒë·∫•u", "k·∫øt qu·∫£"]
    },
    "music": {
        "keywords": ['music', '√¢m nh·∫°c', 'b√†i h√°t', 'ca sƒ©', 'album', 'mv', 'concert', 'lyrics', 'l·ªùi b√†i h√°t', 'spotify', 'apple music'],
        "suffixes": ["new song", "album review", "music video", "tour dates", "lyrics meaning", "b√†i h√°t m·ªõi"]
    },
    "celebrity_gossip": {
        "keywords": ['celebrity', 'ng∆∞·ªùi n·ªïi ti·∫øng', 'showbiz', 'tin ƒë·ªìn', 'scandal', 'drama', 'di·ªÖn vi√™n', 'ca sƒ©'],
        "suffixes": ["scandal", "news", "gossip", "drama", "ph·ªët", "tin ƒë·ªìn"]
    },
    "books_literature": {
        "keywords": ['book', 's√°ch', 'ti·ªÉu thuy·∫øt', 't√°c gi·∫£', 'vƒÉn h·ªçc', 'truy·ªán', 'poetry', 'author', 'novel', 'ƒë·ªçc s√°ch'],
        "suffixes": ["review", "summary", "recommendations", "new releases", "ƒë√°nh gi√° s√°ch", "t√≥m t·∫Øt"]
    },
    "photography_video": {
        "keywords": ['photography', 'nhi·∫øp ·∫£nh', 'quay phim', 'm√°y ·∫£nh', 'camera', 'lens', 'drone', 'ch·ª•p ·∫£nh', 'edit video'],
        "suffixes": ["tutorial", "gear review", "best settings", "tips and tricks", "h∆∞·ªõng d·∫´n", "ƒë√°nh gi√° thi·∫øt b·ªã"]
    },
    "diy_crafts": {
        "keywords": ['diy', 't·ª± l√†m', 'th·ªß c√¥ng', 'handmade', 'craft', 'tutorial', 'h∆∞·ªõng d·∫´n', 'ƒë·ªì handmade'],
        "suffixes": ["how to", "tutorial", "ideas", "project", "h∆∞·ªõng d·∫´n l√†m", "√Ω t∆∞·ªüng"]
    },
    "social_media_trends": {
        "keywords": ['social media', 'm·∫°ng x√£ h·ªôi', 'tiktok', 'instagram', 'facebook', 'twitter', 'viral', 'meme', 'trend', 'xu h∆∞·ªõng'],
        "suffixes": ["new trend", "viral video", "meme explained", "challenge", "xu h∆∞·ªõng m·ªõi", "tr√†o l∆∞u"]
    },
    # --- Lifestyle & Wellness (14-20) ---
    "food_cooking": {
        "keywords": ['food', 'cooking', 'recipe', 'c√¥ng th·ª©c', 'n·∫•u ƒÉn', 'nh√† h√†ng', 'qu√°n ƒÉn', '·∫©m th·ª±c', 'm√≥n ngon'],
        "suffixes": ["recipe", "how to make", "best restaurants", "review", "c√°ch l√†m", "ƒë·ªãa ch·ªâ"]
    },
    "travel": {
        "keywords": ['travel', 'du l·ªãch', 'ph∆∞·ª£t', 'kh√°ch s·∫°n', 'resort', 'v√© m√°y bay', 'ƒë·ªãa ƒëi·ªÉm', 'kinh nghi·ªám'],
        "suffixes": ["travel guide", "things to do", "best places to visit", "flight deals", "kinh nghi·ªám du l·ªãch", "gi√° v√©"]
    },
    "health_wellness": {
        "keywords": ['health', 'wellness', 's·ª©c kh·ªèe', 'fitness', 'gym', 'yoga', 'meditation', 'dinh d∆∞·ª°ng', 'b·ªánh'],
        "suffixes": ["benefits", "how to", "symptoms", "treatment", "healthy diet", "l·ª£i √≠ch", "c√°ch t·∫≠p"]
    },
    "mental_health": {
        "keywords": ['mental health', 's·ª©c kh·ªèe tinh th·∫ßn', 't√¢m l√Ω', 'stress', 'anxiety', 'therapy', 'tr·ªã li·ªáu', 't√¢m s·ª±'],
        "suffixes": ["how to cope", "symptoms of", "self-care tips", "therapy options", "c√°ch ƒë·ªëi ph√≥", "l·ªùi khuy√™n"]
    },
    "fashion_beauty": {
        "keywords": ['fashion', 'th·ªùi trang', 'l√†m ƒë·∫πp', 'beauty', 'm·ªπ ph·∫©m', 'qu·∫ßn √°o', 'brand', 'style', 'makeup', 'ph·ªëi ƒë·ªì'],
        "suffixes": ["trends", "style guide", "product review", "tutorial", "xu h∆∞·ªõng", "c√°ch ph·ªëi ƒë·ªì"]
    },
    "home_garden": {
        "keywords": ['home', 'garden', 'nh√† c·ª≠a', 's√¢n v∆∞·ªùn', 'trang tr√≠', 'n·ªôi th·∫•t', 'diy', 'gardening', 'c√¢y c·∫£nh'],
        "suffixes": ["decor ideas", "gardening tips", "diy project", "organization hacks", "√Ω t∆∞·ªüng trang tr√≠", "m·∫πo l√†m v∆∞·ªùn"]
    },
    "pets_animals": {
        "keywords": ['pet', 'animal', 'th√∫ c∆∞ng', 'ch√≥', 'm√®o', 'dog', 'cat', 'ƒë·ªông v·∫≠t', 'chƒÉm s√≥c th√∫ c∆∞ng'],
        "suffixes": ["care tips", "breeds", "funny videos", "health problems", "c√°ch chƒÉm s√≥c", "gi·ªëng lo√†i"]
    },
    # --- Practical & Professional (21-27) ---
    "education": {
        "keywords": ['education', 'gi√°o d·ª•c', 'h·ªçc t·∫≠p', 'school', 'university', 'tr∆∞·ªùng h·ªçc', 'ƒë·∫°i h·ªçc', 'kh√≥a h·ªçc', 'online course'],
        "suffixes": ["best courses", "how to learn", "study tips", "admission requirements", "kh√≥a h·ªçc t·ªët nh·∫•t", "m·∫πo h·ªçc t·∫≠p"]
    },
    "career_development": {
        "keywords": ['career', 's·ª± nghi·ªáp', 'ph√°t tri·ªÉn b·∫£n th√¢n', 'job search', 't√¨m vi·ªác', 'resume', 'cv', 'interview', 'ph·ªèng v·∫•n'],
        "suffixes": ["job search tips", "resume template", "interview questions", "career path", "m·∫πo t√¨m vi·ªác", "c√¢u h·ªèi ph·ªèng v·∫•n"]
    },
    "business_entrepreneurship": {
        "keywords": ['business', 'kinh doanh', 'kh·ªüi nghi·ªáp', 'startup', 'marketing', 'sales', 'doanh nghi·ªáp'],
        "suffixes": ["business ideas", "how to start", "marketing strategy", "case study", "√Ω t∆∞·ªüng kinh doanh", "chi·∫øn l∆∞·ª£c marketing"]
    },
    "automotive": {
        "keywords": ['automotive', 'xe h∆°i', '√¥ t√¥', 'xe m√°y', 'car', 'motorcycle', 'vehicle', 'xe ƒëi·ªán', 'vinfast'],
        "suffixes": ["review", "specs", "price", "release date", "vs", "ƒë√°nh gi√° xe", "gi√° b√°n"]
    },
    "law_politics": {
        "keywords": ['law', 'politics', 'lu·∫≠t', 'ch√≠nh tr·ªã', 'ch√≠nh ph·ªß', 'government', 'policy', 'election', 'b·∫ßu c·ª≠', 'quy ƒë·ªãnh'],
        "suffixes": ["new law", "policy explained", "election results", "legal advice", "lu·∫≠t m·ªõi", "gi·∫£i th√≠ch ch√≠nh s√°ch"]
    },
    "real_estate": {
        "keywords": ['real estate', 'b·∫•t ƒë·ªông s·∫£n', 'nh√† ƒë·∫•t', 'housing market', 'apartment', 'cƒÉn h·ªô', 'chung c∆∞', 'gi√° nh√†'],
        "suffixes": ["market trends", "how to buy", "investment tips", "apartment tour", "xu h∆∞·ªõng th·ªã tr∆∞·ªùng", "kinh nghi·ªám mua nh√†"]
    },
    "cryptocurrency_blockchain": {
        "keywords": ['crypto', 'bitcoin', 'ethereum', 'blockchain', 'nft', 'defi', 'web3', 'ti·ªÅn ·∫£o', 'ti·ªÅn ƒëi·ªán t·ª≠'],
        "suffixes": ["price prediction", "news", "how to buy", "wallet", "d·ª± ƒëo√°n gi√°", "tin t·ª©c crypto"]
    },
    # --- Local & Shopping (28-31) ---
    "local_events": {
        "keywords": ['event', 's·ª± ki·ªán', 'l·ªÖ h·ªôi', 'concert', 'workshop', 'h·ªôi th·∫£o', 'g·∫ßn ƒë√¢y', 'quanh ƒë√¢y'],
        "suffixes": ["events near me", "tickets", "schedule", "local festivals", "s·ª± ki·ªán s·∫Øp t·ªõi", "l·ªãch tr√¨nh"]
    },
    "shopping_deals": {
        "keywords": ['shopping', 'mua s·∫Øm', 'deal', 'gi·∫£m gi√°', 'khuy·∫øn m√£i', 'sale', 'discount', 'black friday', 'shopee', 'lazada'],
        "suffixes": ["best deals", "discount codes", "sale on", "product review", "m√£ gi·∫£m gi√°", "ƒë√°nh gi√° s·∫£n ph·∫©m"]
    },
    "history": {
        "keywords": ['history', 'l·ªãch s·ª≠', 'chi·∫øn tranh', 'ancient', 'medieval', 'modern history', 'l·ªãch s·ª≠ vi·ªát nam'],
        "suffixes": ["history of", "explained", "documentary", "key events", "l·ªãch s·ª≠ v·ªÅ", "gi·∫£i th√≠ch"]
    },
    "environment_sustainability": {
        "keywords": ['environment', 'm√¥i tr∆∞·ªùng', 'bi·∫øn ƒë·ªïi kh√≠ h·∫≠u', 'climate change', 'sustainability', 'nƒÉng l∆∞·ª£ng t√°i t·∫°o', '√¥ nhi·ªÖm'],
        "suffixes": ["latest news", "solutions", "impact of", "how to help", "tin t·ª©c m√¥i tr∆∞·ªùng", "gi·∫£i ph√°p"]
    },
    # --- Default ---
    "general": {
        "keywords": [],  # Default
        "suffixes": ["news", "latest", "update", "information", "tin t·ª©c", "th√¥ng tin", "m·ªõi nh·∫•t"]
    }
}

async def cached_search(key: str, func: Any, *args: Any) -> Any:
    async with CACHE_LOCK:
        if key in SEARCH_CACHE and datetime.now() - SEARCH_CACHE[key]['time'] < timedelta(hours=6):
            return SEARCH_CACHE[key]['result']
        result = await func(*args)
        SEARCH_CACHE[key] = {'result': result, 'time': datetime.now()}
        return result

async def run_search_apis(query: str, mode: str = "general") -> str:
    logger.info(f"CALLING 3x CSE SMART SEARCH for '{query}' (mode: {mode})")
    global SEARCH_API_COUNTER

    FORCE_FALLBACK_REQUEST = "[FORCE FALLBACK]" in query.upper()
    q_base = query.replace("[FORCE FALLBACK]", "").strip()
    
    sub_queries = []
    if " v√† " in q_base or " and " in q_base.lower() or "," in q_base:
        splitters = re.split(r"\s*(?:v√†|and|,)\s*", q_base, flags=re.IGNORECASE)
        sub_queries = [q.strip() for q in splitters if q.strip()]
    else:
        sub_queries = [q_base.strip()]

    final_results = []

    for q_sub in sub_queries:
        async with SEARCH_LOCK:
            # 1. Ph√¢n lo·∫°i ch·ªß ƒë·ªÅ
            query_lower = q_sub.lower()
            selected_topic = "general"
            for topic, data in SEARCH_TOPICS.items():
                if topic == "general":
                    continue
                if any(keyword in query_lower for keyword in data["keywords"]):
                    selected_topic = topic
                    break
            
            logger.info(f"Ph√¢n lo·∫°i: {selected_topic.upper()}. Ch·∫°y search cho: '{q_sub}'")

            # 2. T·∫°o c√°c truy v·∫•n ƒëa d·∫°ng d·ª±a tr√™n ch·ªß ƒë·ªÅ
            suffixes = SEARCH_TOPICS[selected_topic]["suffixes"]
            random.shuffle(suffixes)
            
            q1 = q_sub.strip()
            q2 = f"{q1} {suffixes[0]} OR {suffixes[1]}" if len(suffixes) > 1 else q1
            q3 = f"{q1} {suffixes[2]} OR {suffixes[3]}" if len(suffixes) > 3 else q1
            
            # Fallback query in case the specialized ones fail
            fallback_q = f"{q_sub.strip()} {SEARCH_TOPICS['general']['suffixes'][0]} OR {SEARCH_TOPICS['general']['suffixes'][1]}"

            logger.info(f"Queries: Q1='{q1}', Q2='{q2}', Q3='{q3}'")

            # --- B·∫ÆT ƒê·∫¶U CH·∫†Y SEARCH ---
            cse0_task = asyncio.create_task(_search_cse(q1, GOOGLE_CSE_ID, GOOGLE_CSE_API_KEY, 0, start_idx=1, force_lang="vi"))
            cse1_task = asyncio.create_task(_search_cse(q2, GOOGLE_CSE_ID_1, GOOGLE_CSE_API_KEY_1, 1, start_idx=1, force_lang="en"))
            cse2_task = asyncio.create_task(_search_cse(q3, GOOGLE_CSE_ID_2, GOOGLE_CSE_API_KEY_2, 2, start_idx=1, force_lang="en"))

            cse0_result, cse1_result, cse2_result = await asyncio.gather(
                cse0_task, cse1_task, cse2_task, return_exceptions=True
            )

            def safe_result(r, name):
                if isinstance(r, Exception):
                    logger.error(f"{name} l·ªói: {r}")
                    return ""
                return r or ""

            cse0_result = safe_result(cse0_result, "CSE0")
            cse1_result = safe_result(cse1_result, "CSE1")
            cse2_result = safe_result(cse2_result, "CSE2")

            # --- LOGIC FALLBACK ---
            # If all CSE results are empty, or forced, run fallback
            should_run_fallback = FORCE_FALLBACK_REQUEST or not (cse0_result or cse1_result or cse2_result)
            
            fallback_result = ""
            if should_run_fallback:
                log_message = "AI y√™u c·∫ßu [FORCE FALLBACK]" if FORCE_FALLBACK_REQUEST else "T·∫•t c·∫£ CSE ƒë·ªÅu r·ªóng/l·ªói"
                logger.warning(f"{log_message} ‚Üí Ch·∫°y Fallback API.")
                
                # Use a more general query for fallback
                fallback_result = await _run_fallback_search(fallback_q)
                if fallback_result:
                    logger.info(f"Fallback th√†nh c√¥ng.")
                else:
                    logger.warning("Fallback th·∫•t b·∫°i.")

            # Combine results
            # Prioritize CSE results, but add fallback if it exists
            parts: list[str] = [str(x) for x in [cse0_result, cse1_result, cse2_result, fallback_result] if x]

            if parts:
                merged = "\n\n".join(parts)
                unique_lines = []
                seen_links = set()
                for line in merged.splitlines():
                    match = re.search(r"\(Ngu·ªìn: (.*?)\)", line)
                    if match:
                        link = match.group(1)
                        if link not in seen_links:
                            seen_links.add(link)
                            unique_lines.append(line)
                    else:
                        unique_lines.append(line)
                final_text = "\n".join(unique_lines)
                final_results.append(f"### üîç [Ch·ªß ƒë·ªÅ: {selected_topic.upper()}] K·∫øt qu·∫£ cho '{q_sub}':\n{final_text.strip()}")

    if final_results:
        logger.info(f"Ho√†n t·∫•t t√¨m ki·∫øm {len(final_results)} subquery.")
        return "\n\n".join(final_results)

    logger.error("T·∫§T C·∫¢ 3 CSE + fallback FAIL.")
    return ""

async def _search_cse(query: str, cse_id: str | None, api_key: str | None, index: int = 0, start_idx: int = 1, force_lang: str | None = None) -> str:
    if not cse_id or not api_key:
        logger.warning(f"CSE{index} ch∆∞a c·∫•u h√¨nh ID/API key.")
        return ""

    params = {
        "key": api_key,
        "cx": cse_id,
        "q": query,
        "num": 3,
        "start": start_idx,
        "gl": "vn",
        # S·ª¨A ƒê·ªîI: D√πng force_lang n·∫øu c√≥, n·∫øu kh√¥ng th√¨ d√πng logic c≈©.
        "hl": force_lang or ("en" if re.search(r"[a-zA-Z]{4,}", query) else "vi"),
    }

    try:
        response = await asyncio.to_thread(
            requests.get,
            "https://www.googleapis.com/customsearch/v1",
            params=params,
            timeout=10,
        )
        data = response.json()

        if "items" not in data:
            logger.warning(f"CSE{index} kh√¥ng c√≥ k·∫øt qu·∫£ h·ª£p l·ªá cho query '{query[:60]}'")
            return ""

        relevant = []
        for item in data["items"][:3]:
            title = item.get("title", "Kh√¥ng c√≥ ti√™u ƒë·ªÅ")
            snippet_raw = item.get("snippet", "")
            snippet = snippet_raw[:330] + "..." if len(snippet_raw) > 130 else snippet_raw
            link = item.get("link", "")
            if any(ad in link.lower() for ad in ["shopee", "lazada", "amazon", "tiki"]):
                continue
            relevant.append(f"**{title}**: {snippet} (Ngu·ªìn: {link})")

        if relevant:
            logger.info(f"CSE{index} tr·∫£ v·ªÅ {len(relevant)} k·∫øt qu·∫£ h·ª£p l·ªá.")
            return f"**Search CSE{index} (Dynamic):**\n" + "\n".join(relevant) + "\n\n[D√ôNG ƒê·ªÇ TR·∫¢ L·ªúI E-GIRL, KH√îNG LEAK NGU·ªíN]"
        return ""

    except Exception as e:
        logger.error(f"CSE{index} l·ªói khi g·ªçi API: {e}")
        return ""

async def _run_fallback_search(query: str) -> str:
    apis = ["SerpAPI", "Tavily", "Exa"]
    global SEARCH_API_COUNTER
    start_idx = SEARCH_API_COUNTER % 3
    SEARCH_API_COUNTER += 1

    for i in range(3):
        api_name = apis[(start_idx + i) % 3]
        try:
            if api_name == "SerpAPI" and SERPAPI_API_KEY:
                result = await _search_serpapi(query)
            elif api_name == "Tavily" and TAVILY_API_KEY:
                result = await _search_tavily(query)
            elif api_name == "Exa" and EXA_API_KEY:
                result = await _search_exa(query)
            else:
                continue

            if result:
                logger.info(f"Fallback {api_name} th√†nh c√¥ng cho query '{query[:60]}'")
                return result
            else:
                logger.warning(f"Fallback {api_name} r·ªóng ho·∫∑c l·ªói.")
        except Exception as e:
            logger.warning(f"Fallback {api_name} l·ªói: {e}")

    logger.error("T·∫§T C·∫¢ fallback APIs ƒë·ªÅu th·∫•t b·∫°i.")
    return ""

async def _search_serpapi(query: str) -> str:
    if not SERPAPI_API_KEY: return ""
    
    params = {
        "q": query,
        "api_key": SERPAPI_API_KEY,
        "engine": "google",
        "num": 3,
        "gl": "vn",
        "hl": "en" if re.search(r'[a-zA-Z]{4,}', query) else "vi"
    }
    
    search = GoogleSearch(params)
    results = await asyncio.to_thread(search.get_dict)
    
    if 'organic_results' not in results:
        return ""
    
    relevant = []
    for item in results['organic_results'][:3]:
        title = item.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')
        snippet = item.get('snippet', '')[:330] + "..." if len(item.get('snippet', '')) > 130 else item.get('snippet', '')
        link = item.get('link', '')
        if any(ad in link.lower() for ad in ['shopee', 'lazada', 'amazon', 'tiki']): continue
        relevant.append(f"**{title}**: {snippet} (Ngu·ªìn: {link})")
    
    return "**Search SerpAPI (Dynamic):**\n" + "\n".join(relevant) + "\n\n[D√ôNG ƒê·ªÇ TR·∫¢ L·ªúI E-GIRL, KH√îNG LEAK NGU·ªíN]" if relevant else ""

async def _search_tavily(query: str) -> str:
    if not TAVILY_API_KEY: return ""
    
    tavily = TavilyClient(api_key=TAVILY_API_KEY)
    params = {
        "query": query,
        "search_depth": "basic",
        "max_results": 3,
        "include_answer": False
    }
    
    results = await asyncio.to_thread(tavily.search, **params)
    
    if 'results' not in results:
        return ""
    
    relevant = []
    for item in results['results'][:3]:
        title = item.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')
        snippet = item.get('content', '')[:330] + "..." if len(item.get('content', '')) > 130 else item.get('content', '')
        link = item.get('url', '')
        if any(ad in link.lower() for ad in ['shopee', 'lazada', 'amazon', 'tiki']): continue
        relevant.append(f"**{title}**: {snippet} (Ngu·ªìn: {link})")
    
    return "**Search Tavily (Dynamic):**\n" + "\n".join(relevant) + "\n\n[D√ôNG ƒê·ªÇ TR·∫¢ L·ªúI E-GIRL, KH√îNG LEAK NGU·ªíN]" if relevant else ""

async def _search_exa(query: str) -> str:
    if not EXA_API_KEY: return ""
    
    exa = exa_py.Exa(api_key=EXA_API_KEY)
    params = {
        "query": query,
        "num_results": 3,
        "use_autoprompt": True,
        "type": "neural"
    }
    
    results = await asyncio.to_thread(exa.search, **params)
    
    if not results.results:
        return ""
    
    relevant = []
    for item in results.results[:3]:
        title = item.title or 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ'
        text = item.text or ''
        snippet = text[:330] + "..." if len(text) > 130 else text
        link = item.url
        if any(ad in link.lower() for ad in ['shopee', 'lazada', 'amazon', 'tiki']): continue
        relevant.append(f"**{title}**: {snippet} (Ngu·ªìn: {link})")
    
    return "**Search Exa.ai (Dynamic):**\n" + "\n".join(relevant) + "\n\n[D√ôNG ƒê·ªÇ TR·∫¢ L·ªúI E-GIRL, KH√îNG LEAK NGU·ªíN]" if relevant else ""

