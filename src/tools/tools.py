import asyncio
import json
import re
import os
import random
from datetime import datetime, timedelta
import aiofiles
import requests
import sympy as sp
from google.generativeai.types import Tool, FunctionDeclaration
import serpapi
from tavily import TavilyClient
import exa_py
import aiohttp

from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

from src.core.config import (
    logger,
    WEATHER_API_KEY,
    CITY,
    WEATHER_CACHE_PATH,
    SERPAPI_API_KEY,
    TAVILY_API_KEY,
    EXA_API_KEY,
    GOOGLE_CSE_ID,
    GOOGLE_CSE_API_KEY,
    GOOGLE_CSE_ID_1,
    GOOGLE_CSE_API_KEY_1,
    GOOGLE_CSE_ID_2,
    GOOGLE_CSE_API_KEY_2,
    HF_TOKEN
)
from dotenv import load_dotenv

load_dotenv()


class ToolsManager:
    """Manager for all AI tools and external API integrations."""
    
    # Cache configuration
    CACHE_TTL_SECONDS = 3600
    MAX_CACHE_SIZE = 1000
    
    # File size limits
    MAX_FILE_SIZE_BYTES = 20 * 1024 * 1024
    MAX_TEXT_LENGTH = 10000
    
    # Google Drive
    SCOPES = ['https://www.googleapis.com/auth/drive.readonly']
    SERVICE_ACCOUNT_FILE = 'credentials.json'
    
    # Search Topics
    SEARCH_TOPICS = {
        "gaming": {
            "keywords": ['game', 'patch', 'banner', 'update', 'release date', 'roadmap', 'leak', 'speculation', 'gacha', 'reroll', 'tier list', 'build', 'nh√¢n v·∫≠t', 'honkai', 'hsr', 'star rail', 'genshin', 'zzz', 'zenless', 'wuwa', 'wuthering waves', 'arknights', 'fgo', 'phi√™n b·∫£n', 's·ª± ki·ªán'],
            "suffixes": ["update", "release date", "patch notes", "roadmap", "leaks", "speculation", "official", "tin t·ª©c"]
        },
        "tech": {
            "keywords": ['tech', 'c√¥ng ngh·ªá', 'ai', 'ios', 'android', 'app', 'software', 'hardware', 'card m√†n h√¨nh', 'cpu', 'laptop', 'phone'],
            "suffixes": ["review", "release date", "news", "vs", "benchmark", "specs", "ƒë√°nh gi√°", "tin t·ª©c"]
        },
        "science": {
            "keywords": ['science', 'khoa h·ªçc', 'space', 'v≈© tr·ª•', 'nasa', 'discovery', 'research', 'nghi√™n c·ª©u', 'y t·∫ø'],
            "suffixes": ["new discovery", "latest research", "breakthrough", "study finds", "c√¥ng b·ªë", "nghi√™n c·ª©u m·ªõi"]
        },
        "finance": {
            "keywords": ['finance', 't√†i ch√≠nh', 'stock', 'c·ªï phi·∫øu', 'market', 'th·ªã tr∆∞·ªùng', 'investment', 'ƒë·∫ßu t∆∞', 'economy', 'kinh t·∫ø', 'l√£i su·∫•t', 'ng√¢n h√†ng'],
            "suffixes": ["stock price", "market analysis", "forecast", "news", "earnings report", "ph√¢n t√≠ch", "d·ª± b√°o"]
        },
        "movies_tv": {
            "keywords": ['movie', 'phim', 'tv show', 'series', 'netflix', 'disney+', 'trailer', 'actor', 'di·ªÖn vi√™n', 'ƒë·∫°o di·ªÖn', 'l·ªãch chi·∫øu'],
            "suffixes": ["review", "release date", "trailer", "cast", "ending explained", "season 2", "l·ªãch chi·∫øu phim", "ƒë√°nh gi√°"]
        },
        "anime_manga": {
            "keywords": ['anime', 'manga', 'light novel', 'manhwa', 'manhua', 'chapter', 'episode', 'season', 'ova', 'ph·∫ßn m·ªõi', 't·∫≠p m·ªõi'],
            "suffixes": ["release date", "new season", "chapter review", "discussion", "spoiler", "tin t·ª©c anime"]
        },
        "sports": {
            "keywords": ['sports', 'th·ªÉ thao', 'b√≥ng ƒë√°', 'football', 'basketball', 'tennis', 'c·∫ßu l√¥ng', 'f1', 'ƒë·ªôi tuy·ªÉn', 'c·∫ßu th·ªß', 'tr·∫≠n ƒë·∫•u'],
            "suffixes": ["match result", "highlights", "live score", "news", "transfer", "l·ªãch thi ƒë·∫•u", "k·∫øt qu·∫£"]
        },
        "music": {
            "keywords": ['music', '√¢m nh·∫°c', 'b√†i h√°t', 'ca sƒ©', 'album', 'mv', 'concert', 'lyrics', 'l·ªùi b√†i h√°t', 'spotify', 'apple music'],
            "suffixes": ["new song", "album review", "music video", "tour dates", "lyrics meaning", "b√†i h√°t m·ªõi"]
        },
        "celebrity_gossip": {
            "keywords": ['celebrity', 'ng∆∞·ªùi n·ªïi ti·∫øng', 'showbiz', 'tin ƒë·ªìn', 'scandal', 'drama', 'di·ªÖn vi√™n', 'ca sƒ©'],
            "suffixes": ["scandal", "news", "gossip", "drama", "ph·ªët", "tin ƒë·ªìn"]
        },
        "books_literature": {
            "keywords": ['book', 's√°ch', 'ti·ªÉu thuy·∫øt', 't√°c gi·∫£', 'vƒÉn h·ªçc', 'truy·ªán', 'poetry', 'author', 'novel', 'ƒë·ªçc s√°ch'],
            "suffixes": ["review", "summary", "recommendations", "new releases", "ƒë√°nh gi√° s√°ch", "t√≥m t·∫Øt"]
        },
        "photography_video": {
            "keywords": ['photography', 'nhi·∫øp ·∫£nh', 'quay phim', 'm√°y ·∫£nh', 'camera', 'lens', 'drone', 'ch·ª•p ·∫£nh', 'edit video'],
            "suffixes": ["tutorial", "gear review", "best settings", "tips and tricks", "h∆∞·ªõng d·∫´n", "ƒë√°nh gi√° thi·∫øt b·ªã"]
        },
        "diy_crafts": {
            "keywords": ['diy', 't·ª± l√†m', 'th·ªß c√¥ng', 'handmade', 'craft', 'tutorial', 'h∆∞·ªõng d·∫´n', 'ƒë·ªì handmade'],
            "suffixes": ["how to", "tutorial", "ideas", "project", "h∆∞·ªõng d·∫´n l√†m", "√Ω t∆∞·ªüng"]
        },
        "social_media_trends": {
            "keywords": ['social media', 'm·∫°ng x√£ h·ªôi', 'tiktok', 'instagram', 'facebook', 'twitter', 'viral', 'meme', 'trend', 'xu h∆∞·ªõng'],
            "suffixes": ["new trend", "viral video", "meme explained", "challenge", "xu h∆∞·ªõng m·ªõi", "tr√†o l∆∞u"]
        },
        "food_cooking": {
            "keywords": ['food', 'cooking', 'recipe', 'c√¥ng th·ª©c', 'n·∫•u ƒÉn', 'nh√† h√†ng', 'qu√°n ƒÉn', '·∫©m th·ª±c', 'm√≥n ngon'],
            "suffixes": ["recipe", "how to make", "best restaurants", "review", "c√°ch l√†m", "ƒë·ªãa ch·ªâ"]
        },
        "travel": {
            "keywords": ['travel', 'du l·ªãch', 'ph∆∞·ª£t', 'kh√°ch s·∫°n', 'resort', 'v√© m√°y bay', 'ƒë·ªãa ƒëi·ªÉm', 'kinh nghi·ªám'],
            "suffixes": ["travel guide", "things to do", "best places to visit", "flight deals", "kinh nghi·ªám du l·ªãch", "gi√° v√©"]
        },
        "health_wellness": {
            "keywords": ['health', 'wellness', 's·ª©c kh·ªèe', 'fitness', 'gym', 'yoga', 'meditation', 'dinh d∆∞·ª°ng', 'b·ªánh'],
            "suffixes": ["benefits", "how to", "symptoms", "treatment", "healthy diet", "l·ª£i √≠ch", "c√°ch t·∫≠p"]
        },
        "mental_health": {
            "keywords": ['mental health', 's·ª©c kh·ªèe tinh th·∫ßn', 't√¢m l√Ω', 'stress', 'anxiety', 'therapy', 'tr·ªã li·ªáu', 't√¢m s·ª±'],
            "suffixes": ["how to cope", "symptoms of", "self-care tips", "therapy options", "c√°ch ƒë·ªëi ph√≥", "l·ªùi khuy√™n"]
        },
        "fashion_beauty": {
            "keywords": ['fashion', 'th·ªùi trang', 'l√†m ƒë·∫πp', 'beauty', 'm·ªπ ph·∫©m', 'qu·∫ßn √°o', 'brand', 'style', 'makeup', 'ph·ªëi ƒë·ªì'],
            "suffixes": ["trends", "style guide", "product review", "tutorial", "xu h∆∞·ªõng", "c√°ch ph·ªëi ƒë·ªì"]
        },
        "home_garden": {
            "keywords": ['home', 'garden', 'nh√† c·ª≠a', 's√¢n v∆∞·ªùn', 'trang tr√≠', 'n·ªôi th·∫•t', 'diy', 'gardening', 'c√¢y c·∫£nh'],
            "suffixes": ["decor ideas", "gardening tips", "diy project", "organization hacks", "√Ω t∆∞·ªüng trang tr√≠", "m·∫πo l√†m v∆∞·ªùn"]
        },
        "pets_animals": {
            "keywords": ['pet', 'animal', 'th√∫ c∆∞ng', 'ch√≥', 'm√®o', 'dog', 'cat', 'ƒë·ªông v·∫≠t', 'chƒÉm s√≥c th√∫ c∆∞ng'],
            "suffixes": ["care tips", "breeds", "funny videos", "health problems", "c√°ch chƒÉm s√≥c", "gi·ªëng lo√†i"]
        },
        "education": {
            "keywords": ['education', 'gi√°o d·ª•c', 'h·ªçc t·∫≠p', 'school', 'university', 'tr∆∞·ªùng h·ªçc', 'ƒë·∫°i h·ªçc', 'kh√≥a h·ªçc', 'online course'],
            "suffixes": ["best courses", "how to learn", "study tips", "admission requirements", "kh√≥a h·ªçc t·ªët nh·∫•t", "m·∫πo h·ªçc t·∫≠p"]
        },
        "career_development": {
            "keywords": ['career', 's·ª± nghi·ªáp', 'ph√°t tri·ªÉn b·∫£n th√¢n', 'job search', 't√¨m vi·ªác', 'resume', 'cv', 'interview', 'ph·ªèng v·∫•n'],
            "suffixes": ["job search tips", "resume template", "interview questions", "career path", "m·∫πo t√¨m vi·ªác", "c√¢u h·ªèi ph·ªèng v·∫•n"]
        },
        "business_entrepreneurship": {
            "keywords": ['business', 'kinh doanh', 'kh·ªüi nghi·ªáp', 'startup', 'marketing', 'sales', 'doanh nghi·ªáp'],
            "suffixes": ["business ideas", "how to start", "marketing strategy", "case study", "√Ω t∆∞·ªüng kinh doanh", "chi·∫øn l∆∞·ª£c marketing"]
        },
        "automotive": {
            "keywords": ['automotive', 'xe h∆°i', '√¥ t√¥', 'xe m√°y', 'car', 'motorcycle', 'vehicle', 'xe ƒëi·ªán', 'vinfast'],
            "suffixes": ["review", "specs", "price", "release date", "vs", "ƒë√°nh gi√° xe", "gi√° b√°n"]
        },
        "law_politics": {
            "keywords": ['law', 'politics', 'lu·∫≠t', 'ch√≠nh tr·ªã', 'ch√≠nh ph·ªß', 'government', 'policy', 'election', 'b·∫ßu c·ª≠', 'quy ƒë·ªãnh'],
            "suffixes": ["new law", "policy explained", "election results", "legal advice", "lu·∫≠t m·ªõi", "gi·∫£i th√≠ch ch√≠nh s√°ch"]
        },
        "real_estate": {
            "keywords": ['real estate', 'b·∫•t ƒë·ªông s·∫£n', 'nh√† ƒë·∫•t', 'housing market', 'apartment', 'cƒÉn h·ªô', 'l·ªãch s·ª≠ gi√° nh√†'],
            "suffixes": ["market trends", "how to buy", "investment tips", "apartment tour", "xu h∆∞·ªõng th·ªã tr∆∞·ªùng", "kinh nghi·ªám mua nh√†"]
        },
        "cryptocurrency_blockchain": {
            "keywords": ['crypto', 'bitcoin', 'ethereum', 'blockchain', 'nft', 'defi', 'web3', 'ti·ªÅn ·∫£o', 'ti·ªÅn ƒëi·ªán t·ª≠'],
            "suffixes": ["price prediction", "news", "how to buy", "wallet", "d·ª± ƒëo√°n gi√°", "tin t·ª©c crypto"]
        },
        "local_events": {
            "keywords": ['event', 's·ª± ki·ªán', 'l·ªÖ h·ªôi', 'concert', 'workshop', 'h·ªôi th·∫£o', 'g·∫ßn ƒë√¢y', 'quanh ƒë√¢y'],
            "suffixes": ["events near me", "tickets", "schedule", "local festivals", "s·ª± ki·ªán s·∫Øp t·ªõi", "l·ªãch tr√¨nh"]
        },
        "shopping_deals": {
            "keywords": ['shopping', 'mua s·∫Øm', 'deal', 'gi·∫£m gi√°', 'khuy·∫øn m√£i', 'sale', 'discount', 'black friday', 'shopee', 'lazada'],
            "suffixes": ["best deals", "discount codes", "sale on", "product review", "m√£ gi·∫£m gi√°", "ƒë√°nh gi√° s·∫£n ph·∫©m"]
        },
        "history": {
            "keywords": ['history', 'l·ªãch s·ª≠', 'chi·∫øn tranh', 'ancient', 'medieval', 'modern history', 'l·ªãch s·ª≠ vi·ªát nam'],
            "suffixes": ["history of", "explained", "documentary", "key events", "l·ªãch s·ª≠ v·ªÅ", "gi·∫£i th√≠ch"]
        },
        "environment_sustainability": {
            "keywords": ['environment', 'm√¥i tr∆∞·ªùng', 'bi·∫øn ƒë·ªïi kh√≠ h·∫≠u', 'climate change', 'sustainability', 'nƒÉng l∆∞·ª£ng t√°i t·∫°o', '√¥ nhi·ªÖm'],
            "suffixes": ["latest news", "solutions", "impact of", "how to help", "tin t·ª©c m√¥i tr∆∞·ªùng", "gi·∫£i ph√°p"]
        },
        "general": {
            "keywords": [],
            "suffixes": ["news", "latest", "update", "information", "tin t·ª©c", "th√¥ng tin", "m·ªõi nh·∫•t"]
        }
    }
    
    # City mapping
    CITY_NAME_MAP = {
        "h·ªì ch√≠ minh": ("Ho Chi Minh City", "Th√†nh ph·ªë H·ªì Ch√≠ Minh"),
        "tp.hcm": ("Ho Chi Minh City", "Th√†nh ph·ªë H·ªì Ch√≠ Minh"),
        "s√†i g√≤n": ("Ho Chi Minh City", "Th√†nh ph·ªë H·ªì Ch√≠ Minh"),
        "ho chi minh city": ("Ho Chi Minh City", "Th√†nh ph·ªë H·ªì Ch√≠ Minh"),
        "hcmc": ("Ho Chi Minh City", "Th√†nh ph·ªë H·ªì Ch√≠ Minh"),
        "h√† n·ªôi": ("Hanoi", "H√† N·ªôi"),
        "ha noi": ("Hanoi", "H√† N·ªôi"),
        "danang": ("Da Nang", "ƒê√† N·∫µng"),
        "ƒë√† n·∫µng": ("Da Nang", "ƒê√† N·∫µng"),
        "da nang": ("Da Nang", "ƒê√† N·∫µng"),
    }
    
    def __init__(self):
        self.logger = logger
        self.web_search_cache = {}
        self.image_recognition_cache = {}
        self.weather_lock = asyncio.Lock()
        self.search_api_counter = 0
        self.search_lock = asyncio.Lock()
        self.cache_lock = asyncio.Lock()
        self.search_cache = {}
    
    def get_all_tools(self):
        """Return all tool definitions for Gemini."""
        return [
            Tool(function_declarations=[
                FunctionDeclaration(
                    name="web_search",
                    description=(
                        "T√¨m ki·∫øm th√¥ng tin c·∫≠p nh·∫≠t, s·ª± ki·ªán m·ªõi, tin t·ª©c, "
                        "d·ªØ li·ªáu th·ª±c t·∫ø kh√¥ng c√≥ trong ki·∫øn th·ª©c c·ªßa AI, "
                        "ho·∫∑c ƒë·ªÉ x√°c minh th√¥ng tin. KH√îNG D√ôNG cho c√°c t√°c v·ª• t√≠nh to√°n, "
                        "d·ªãch thu·∫≠t, t√≥m t·∫Øt, vi·∫øt l·∫°i, ho·∫∑c c√°c c√¢u h·ªèi kh√¥ng c·∫ßn d·ªØ li·ªáu m·ªõi."
                    ),
                )
            ]),
            Tool(function_declarations=[
                FunctionDeclaration(
                    name="get_weather",
                    description="L·∫•y th√¥ng tin th·ªùi ti·∫øt hi·ªán t·∫°i cho m·ªôt th√†nh ph·ªë c·ª• th·ªÉ.",
                    parameters={
                        "type": "object",
                        "properties": {"city": {"type": "string", "description": "T√™n th√†nh ph·ªë, v√≠ d·ª•: 'Hanoi', 'Tokyo'."}},
                        "required": ["city"]
                    }
                )
            ]),
            Tool(function_declarations=[
                FunctionDeclaration(
                    name="calculate",
                    description="Gi·∫£i c√°c b√†i to√°n s·ªë h·ªçc ho·∫∑c bi·ªÉu th·ª©c ph·ª©c t·∫°p, bao g·ªìm c√°c h√†m l∆∞·ª£ng gi√°c, logarit, v√† ƒë·∫°i s·ªë.",
                    parameters={
                        "type": "object",
                        "properties": {"equation": {"type": "string", "description": "Bi·ªÉu th·ª©c to√°n h·ªçc d∆∞·ªõi d·∫°ng string, v√≠ d·ª•: 'sin(pi/2) + 2*x'."}},
                        "required": ["equation"]
                    }
                )
            ]),
            Tool(function_declarations=[
                FunctionDeclaration(
                    name="save_note",
                    description=(
                        "L∆∞u m·ªôt m·∫©u th√¥ng tin, s·ªü th√≠ch, s·ª± th·∫≠t, ho·∫∑c n·ªôi dung quan tr·ªçng v·ªÅ ng∆∞·ªùi d√πng ƒë·ªÉ b·∫°n c√≥ th·ªÉ truy c·∫≠p l·∫°i sau. "
                        "D√πng khi user chia s·∫ª th√¥ng tin c√° nh√¢n c√≥ gi√° tr·ªã l√¢u d√†i (v√≠ d·ª•: 't√¥i th√≠ch ch∆°i game X', 'c·∫•u h√¨nh m√°y c·ªßa t√¥i l√† Y')."
                    ),
                    parameters={
                        "type": "object",
                        "properties": {
                            "note_content": {"type": "string", "description": "N·ªôi dung th√¥ng tin c·∫ßn ghi nh·ªõ."},
                            "source": {"type": "string", "description": "Ng·ªØ c·∫£nh ho·∫∑c ngu·ªìn c·ªßa th√¥ng tin, v√≠ d·ª•: 'chat_inference', 'user_request'."}
                        },
                        "required": ["note_content", "source"]
                    }
                )
            ]),
            Tool(function_declarations=[
                FunctionDeclaration(
                    name="retrieve_notes",
                    description=(
                        "Truy xu·∫•t th√¥ng tin ƒë√£ l∆∞u tr·ªØ v·ªÅ ng∆∞·ªùi d√πng, ki·∫øn th·ª©c c√° nh√¢n, "
                        "d·ªØ li·ªáu l·ªãch s·ª≠, ho·∫∑c c√°c s·ª± ki·ªán/th√¥ng tin quan tr·ªçng m√† AI ƒë√£ ƒë∆∞·ª£c y√™u c·∫ßu ghi nh·ªõ. "
                        "PH·∫¢I LU√îN G·ªåI H√ÄM N√ÄY n·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn ki·∫øn th·ª©c c√° nh√¢n, note, ho·∫∑c th√¥ng tin ƒë√£ ƒë∆∞·ª£c ghi nh·ªõ tr∆∞·ªõc ƒë√≥."
                    ),
                    parameters={
                        "type": "object",
                        "properties": {
                            "query": {"type": "string", "description": "T·ª´ kh√≥a ho·∫∑c ch·ªß ƒë·ªÅ t√¨m ki·∫øm trong b·ªô nh·ªõ (v√≠ d·ª•: 'config', 's·ªü th√≠ch'). ƒê·ªÉ tr·ªëng n·∫øu mu·ªën l·∫•y t·∫•t c·∫£."}
                        },
                        "required": ["query"]
                    }
                )
            ]),
            Tool(function_declarations=[
                FunctionDeclaration(
                    name="image_recognition",
                    description=(
                        "Nh·∫≠n di·ªán ƒë·ªëi t∆∞·ª£ng, ng∆∞·ªùi n·ªïi ti·∫øng, nh√¢n v·∫≠t game/anime, ƒë·∫øm v·∫≠t th·ªÉ, v√† tr√≠ch xu·∫•t vƒÉn b·∫£n (OCR) t·ª´ m·ªôt h√¨nh ·∫£nh. "
                        "S·ª≠ d·ª•ng khi ng∆∞·ªùi d√πng t·∫£i l√™n m·ªôt h√¨nh ·∫£nh v√† h·ªèi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn n·ªôi dung c·ªßa h√¨nh ·∫£nh ƒë√≥. "
                        "V√≠ d·ª•: 'c√≥ bao nhi√™u qu·∫£ t√°o trong ·∫£nh?', 'ng∆∞·ªùi n√†y l√† ai?', 'ƒë√¢y l√† nh√¢n v·∫≠t g√¨?', 'ƒë·ªçc ch·ªØ trong ·∫£nh n√†y'."
                    ),
                    parameters={
                        "type": "object",
                        "properties": {
                            "image_url": {"type": "string", "description": "URL c√¥ng khai c·ªßa h√¨nh ·∫£nh c·∫ßn nh·∫≠n di·ªán."},
                            "question": {"type": "string", "description": "C√¢u h·ªèi c·ª• th·ªÉ c·ªßa ng∆∞·ªùi d√πng v·ªÅ h√¨nh ·∫£nh (v√≠ d·ª•: 'ƒë·∫øm s·ªë l∆∞·ª£ng', 'ng∆∞·ªùi n√†y l√† ai?', 'ƒë√¢y l√† g√¨?')."}
                        },
                        "required": ["image_url", "question"]
                    }
                )
            ]),
        ]
    
    def get_web_search_cache(self, query: str):
        """Get cached web search result."""
        if query in self.web_search_cache:
            cached_item = self.web_search_cache[query]
            if datetime.now() - cached_item['timestamp'] < timedelta(hours=6):
                return cached_item['data']
            else:
                del self.web_search_cache[query]
        return None
    
    def set_web_search_cache(self, query: str, data: str):
        """Set web search cache."""
        if len(self.web_search_cache) >= self.MAX_CACHE_SIZE:
            oldest_key = min(self.web_search_cache, key=lambda k: self.web_search_cache[k]['timestamp'])
            del self.web_search_cache[oldest_key]
        self.web_search_cache[query] = {'data': data, 'timestamp': datetime.now()}
    
    def get_image_recognition_cache(self, image_url: str, question: str):
        """Get cached image recognition result."""
        key = f"{image_url}|{question}"
        if key in self.image_recognition_cache:
            cached_item = self.image_recognition_cache[key]
            if datetime.now() - cached_item['timestamp'] < timedelta(hours=1):
                return cached_item['data']
            else:
                del self.image_recognition_cache[key]
        return None
    
    def set_image_recognition_cache(self, image_url: str, question: str, data: str):
        """Set image recognition cache."""
        key = f"{image_url}|{question}"
        if len(self.image_recognition_cache) >= self.MAX_CACHE_SIZE:
            oldest_key = min(self.image_recognition_cache, key=lambda k: self.image_recognition_cache[k]['timestamp'])
            del self.image_recognition_cache[oldest_key]
        self.image_recognition_cache[key] = {'data': data, 'timestamp': datetime.now()}
    
    def normalize_city_name(self, city_query: str):
        """Normalize city name to English and Vietnamese."""
        if not city_query:
            return ("Ho Chi Minh City", "Th√†nh ph·ªë H·ªì Ch√≠ Minh")
        city_key = city_query.strip().lower()
        for k, v in self.CITY_NAME_MAP.items():
            if k in city_key:
                return v
        return (city_query, city_query.title())
    
    async def get_weather(self, city_query: str = None):
        """Get weather information for a city."""
        async with self.weather_lock:
            if city_query is None:
                city_query = CITY or "Ho Chi Minh City"
            city_en, city_vi = self.normalize_city_name(city_query)
            
            cache_path = WEATHER_CACHE_PATH.replace(".json", f"_{city_en.replace(' ', '_').lower()}.json")
            
            if await asyncio.to_thread(os.path.exists, cache_path):
                try:
                    async with aiofiles.open(cache_path, 'r') as f:
                        cache = json.loads(await f.read())
                    cache_time = datetime.fromisoformat(cache['timestamp'])
                    if datetime.now() - cache_time < timedelta(hours=1):
                        return {**cache['data'], "city_vi": city_vi}
                except:
                    pass
            
            if not WEATHER_API_KEY:
                default_data = {
                    'current': f'M∆∞a r√†o s√°ng, m√¢y chi·ªÅu ·ªü {city_vi} (23-28¬∞C).',
                    'forecast': [f'Ng√†y mai: N·∫Øng, 26¬∞C', f'Ng√†y kia: M∆∞a, 25¬∞C'] * 3,
                    'timestamp': datetime.now().isoformat(),
                    'city_vi': city_vi
                }
                with open(cache_path, 'w') as f:
                    json.dump({'data': default_data, 'timestamp': datetime.now().isoformat()}, f)
                return default_data
            
            try:
                url = f"http://api.weatherapi.com/v1/forecast.json?key={WEATHER_API_KEY}&q={city_en}&days=7&aqi=no&alerts=no"
                response = requests.get(url, timeout=10)
                if response.status_code != 200:
                    raise ValueError(f"API status: {response.status_code}")
                
                data = response.json()
                if 'error' in data:
                    raise ValueError(f"API error: {data['error']['message']}")
                
                current = data['current']['condition']['text'] + f" ({data['current']['temp_c']}¬∞C)"
                forecast = []
                for day in data['forecast']['forecastday'][1:7]:
                    forecast.append(f"Ng√†y {day['date']}: {day['day']['condition']['text']} ({day['day']['avgtemp_c']}¬∞C)")
                
                weather_data = {
                    'current': current,
                    'forecast': forecast,
                    'timestamp': datetime.now().isoformat(),
                    'city_vi': city_vi
                }
                
                cache_entry = {'data': weather_data, 'timestamp': datetime.now().isoformat()}
                with open(cache_path, 'w') as f:
                    json.dump(cache_entry, f, indent=2)
                
                return weather_data
            except Exception as e:
                self.logger.error(f"Weather API error: {e}")
                fallback_data = {
                    'current': f'L·ªói API, d√πng m·∫∑c ƒë·ªãnh: M∆∞a r√†o ·ªü {city_vi}, 23-28¬∞C.',
                    'forecast': [f'Ng√†y mai: N·∫Øng, 26¬∞C', f'Ng√†y kia: M∆∞a, 25¬∞C'] * 3,
                    'timestamp': datetime.now().isoformat(),
                    'city_vi': city_vi
                }
                with open(cache_path, 'w') as f:
                    json.dump({'data': fallback_data, 'timestamp': datetime.now().isoformat()}, f)
                return fallback_data
    
    def run_calculator(self, equation_str: str):
        """Run mathematical calculation."""
        cleaned_eq = equation_str.strip().lower().replace('x', '*').replace(',', '.')
        if cleaned_eq.endswith('='):
            cleaned_eq = cleaned_eq[:-1]
        
        try:
            expr = sp.sympify(cleaned_eq, evaluate=False)
            result = sp.N(expr)
            result_str = str(result)
            if result_str.endswith('.0'):
                result_str = result_str[:-2]
            return json.dumps({
                "equation": equation_str,
                "result": result_str,
                "success": True
            }, ensure_ascii=False)
        except (sp.SympifyError, TypeError, ZeroDivisionError, Exception) as e:
            return json.dumps({
                "equation": equation_str,
                "result": f"L·ªói bi·ªÉu th·ª©c: {str(e)}",
                "success": False
            }, ensure_ascii=False)
    
    async def run_image_recognition(self, image_url: str, question: str):
        """Run image recognition using Hugging Face API."""
        cached_result = self.get_image_recognition_cache(image_url, question)
        if cached_result:
            self.logger.info(f"Image recognition result from cache for URL: {image_url}, Question: {question[:30]}...")
            return cached_result
        
        if not HF_TOKEN:
            return "L·ªói: Kh√¥ng t√¨m th·∫•y Hugging Face API token. Vui l√≤ng c·∫•u h√¨nh HF_TOKEN trong config.py."
        
        API_URL = "https://router.huggingface.co/v1/chat/completions"
        headers = {"Authorization": f"Bearer {HF_TOKEN}", "Content-Type": "application/json"}
        
        MAX_RETRIES = 5
        INITIAL_BACKOFF_DELAY = 5
        
        for attempt in range(MAX_RETRIES):
            try:
                async with aiohttp.ClientSession() as session:
                    json_payload = {
                        "model": "Qwen/Qwen2.5-VL-7B-Instruct",
                        "messages": [
                            {
                                "role": "user",
                                "content": [
                                    {"type": "image_url", "image_url": {"url": image_url}},
                                    {"type": "text", "text": question}
                                ]
                            }
                        ],
                        "max_tokens": 300
                    }
                    
                    async with session.post(API_URL, headers=headers, json=json_payload) as response:
                        response.raise_for_status()
                        result = await response.json()
                        self.logger.info(f"HF Image Recognition raw result: {result}")
                        
                        if "choices" in result and result["choices"]:
                            generated_text = result["choices"][0]["message"]["content"]
                            assistant_tag = "<|im_start|>assistant\n"
                            if assistant_tag in generated_text:
                                final_result = generated_text.split(assistant_tag, 1)[1].strip()
                            else:
                                final_result = generated_text.strip()
                            
                            self.set_image_recognition_cache(image_url, question, final_result)
                            return final_result
                        
                        error_result = json.dumps(result, ensure_ascii=False)
                        self.set_image_recognition_cache(image_url, question, error_result)
                        return error_result
            
            except aiohttp.ClientResponseError as e:
                if e.status == 429:
                    delay = INITIAL_BACKOFF_DELAY * (2 ** attempt)
                    self.logger.warning(f"HF API rate limit (429). Retrying in {delay:.2f}s... (Attempt {attempt + 1}/{MAX_RETRIES})")
                    await asyncio.sleep(delay)
                else:
                    self.logger.error(f"HF API error ({e.status}): {e.message}")
                    return f"L·ªói t·ª´ d·ªãch v·ª• nh·∫≠n di·ªán h√¨nh ·∫£nh (m√£ {e.status}): {e.message}"
            except aiohttp.ClientError as e:
                self.logger.error(f"Connection error to HF API: {e}")
                return f"L·ªói k·∫øt n·ªëi ƒë·∫øn d·ªãch v·ª• nh·∫≠n di·ªán h√¨nh ·∫£nh: {e}"
            except Exception as e:
                self.logger.error(f"Unknown error in image recognition: {e}")
                return f"ƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën khi x·ª≠ l√Ω h√¨nh ·∫£nh: {e}"
        
        return f"L·ªói: ƒê√£ th·ª≠ l·∫°i {MAX_RETRIES} l·∫ßn nh∆∞ng kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn d·ªãch v·ª• nh·∫≠n di·ªán h√¨nh ·∫£nh do gi·ªõi h·∫°n rate."
    
    async def _search_cse(self, query: str, cse_id: str, api_key: str, index: int = 0, start_idx: int = 1, force_lang: str = None):
        """Search using Google Custom Search Engine."""
        if not cse_id or not api_key:
            self.logger.warning(f"CSE{index} kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh ID/API key.")
            return ""
        
        params = {
            "key": api_key,
            "cx": cse_id,
            "q": query,
            "num": 3,
            "start": start_idx,
            "gl": "vn",
            "hl": force_lang or ("en" if re.search(r"[a-zA-Z]{4,}", query) else "vi"),
        }
        
        try:
            response = await asyncio.to_thread(
                requests.get,
                "https://www.googleapis.com/customsearch/v1",
                params=params,
                timeout=10,
            )
            data = response.json()
            
            if "items" not in data:
                self.logger.warning(f"CSE{index} kh√¥ng c√≥ k·∫øt qu·∫£ h·ª£p l·ªá cho query '{query[:60]}'")
                return ""
            
            relevant = []
            for item in data["items"][:3]:
                title = item.get("title", "Kh√¥ng c√≥ ti√™u ƒë·ªÅ")
                snippet_raw = item.get("snippet", "")
                snippet = snippet_raw[:330] + "..." if len(snippet_raw) > 130 else snippet_raw
                link = item.get("link", "")
                if any(ad in link.lower() for ad in ["shopee", "lazada", "amazon", "tiki"]):
                    continue
                relevant.append(f"**{title}**: {snippet} (Ngu·ªìn: {link})")
            
            if relevant:
                self.logger.info(f"CSE{index} returned {len(relevant)} valid results.")
                return f"**Search CSE{index} (Dynamic):**\n" + "\n".join(relevant) + "\n\n[D√ôNG ƒê·ªÇ TR·∫¢ L·ªúI E-GIRL, KH√îNG LEAK NGU·ªíN]"
            return ""
        
        except Exception as e:
            self.logger.error(f"CSE{index} error calling API: {e}")
            return ""
    
    async def _search_serpapi(self, query: str):
        """Search using SerpAPI."""
        if not SERPAPI_API_KEY:
            return ""
        
        params = {
            "q": query,
            "api_key": SERPAPI_API_KEY,
            "engine": "google",
            "num": 3,
            "gl": "vn",
            "hl": "en" if re.search(r'[a-zA-Z]{4,}', query) else "vi"
        }
        
        results = await asyncio.to_thread(serpapi.search, params)
        
        if 'organic_results' not in results:
            return ""
        
        relevant = []
        for item in results['organic_results'][:3]:
            title = item.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')
            snippet = item.get('snippet', '')[:330] + "..." if len(item.get('snippet', '')) > 130 else item.get('snippet', '')
            link = item.get('link', '')
            if any(ad in link.lower() for ad in ['shopee', 'lazada', 'amazon', 'tiki']):
                continue
            relevant.append(f"**{title}**: {snippet} (Ngu·ªìn: {link})")
        
        return "**Search SerpAPI (Dynamic):**\n" + "\n".join(relevant) + "\n\n[D√ôNG ƒê·ªÇ TR·∫¢ L·ªúI E-GIRL, KH√îNG LEAK NGU·ªíN]" if relevant else ""
    
    async def _search_tavily(self, query: str):
        """Search using Tavily API."""
        if not TAVILY_API_KEY:
            return ""
        
        tavily = TavilyClient(api_key=TAVILY_API_KEY)
        params = {
            "query": query,
            "search_depth": "basic",
            "max_results": 3,
            "include_answer": False
        }
        
        results = await asyncio.to_thread(tavily.search, **params)
        
        if 'results' not in results:
            return ""
        
        relevant = []
        for item in results['results'][:3]:
            title = item.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')
            snippet = item.get('content', '')[:330] + "..." if len(item.get('content', '')) > 130 else item.get('content', '')
            link = item.get('url', '')
            if any(ad in link.lower() for ad in ['shopee', 'lazada', 'amazon', 'tiki']):
                continue
            relevant.append(f"**{title}**: {snippet} (Ngu·ªìn: {link})")
        
        return "**Search Tavily (Dynamic):**\n" + "\n".join(relevant) + "\n\n[D√ôNG ƒê·ªÇ TR·∫¢ L·ªúI E-GIRL, KH√îNG LEAK NGU·ªíN]" if relevant else ""
    
    async def _search_exa(self, query: str):
        """Search using Exa API."""
        if not EXA_API_KEY:
            return ""
        
        exa = exa_py.Exa(api_key=EXA_API_KEY)
        params = {
            "query": query,
            "num_results": 3,
            "use_autoprompt": True,
            "type": "neural"
        }
        
        results = await asyncio.to_thread(exa.search, **params)
        
        if not results.results:
            return ""
        
        relevant = []
        for item in results.results[:3]:
            title = item.title or 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ'
            text = item.text or ''
            snippet = text[:330] + "..." if len(text) > 130 else text
            link = item.url
            if any(ad in link.lower() for ad in ['shopee', 'lazada', 'amazon', 'tiki']):
                continue
            relevant.append(f"**{title}**: {snippet} (Ngu·ªìn: {link})")
        
        return "**Search Exa.ai (Dynamic):**\n" + "\n".join(relevant) + "\n\n[D√ôNG ƒê·ªÇ TR·∫¢ L·ªúI E-GIRL, KH√îNG LEAK NGU·ªíN]" if relevant else ""
    
    async def _run_fallback_search(self, query: str):
        """Run fallback search using alternative APIs."""
        apis = ["SerpAPI", "Tavily", "Exa"]
        start_idx = self.search_api_counter % 3
        self.search_api_counter += 1
        
        for i in range(3):
            api_name = apis[(start_idx + i) % 3]
            try:
                if api_name == "SerpAPI" and SERPAPI_API_KEY:
                    result = await self._search_serpapi(query)
                elif api_name == "Tavily" and TAVILY_API_KEY:
                    result = await self._search_tavily(query)
                elif api_name == "Exa" and EXA_API_KEY:
                    result = await self._search_exa(query)
                else:
                    continue
                
                if result:
                    self.logger.info(f"Fallback {api_name} success for query '{query[:60]}'")
                    return result
                else:
                    self.logger.warning(f"Fallback {api_name} empty or error.")
            except Exception as e:
                self.logger.warning(f"Fallback {api_name} error: {e}")
        
        self.logger.error("All fallback APIs failed.")
        return ""
    
    async def run_search_apis(self, query: str, mode: str = "general"):
        """Run web search with CSE and fallback APIs."""
        cached_result = self.get_web_search_cache(query)
        if cached_result:
            self.logger.info(f"Web search result from cache for query: {query[:50]}...")
            return cached_result
        
        self.logger.info(f"CALLING 3x CSE SMART SEARCH for '{query}' (mode: {mode})")
        
        FORCE_FALLBACK_REQUEST = "[FORCE FALLBACK]" in query.upper()
        q_base = query.replace("[FORCE FALLBACK]", "").strip()
        
        sub_queries = []
        if " v√† " in q_base or " and " in q_base.lower() or "," in q_base:
            splitters = re.split(r"\s*(?:v√†|and|,)\s*", q_base, flags=re.IGNORECASE)
            sub_queries = [q.strip() for q in splitters if q.strip()]
        else:
            sub_queries = [q_base.strip()]
        
        final_results = []
        
        for q_sub in sub_queries:
            async with self.search_lock:
                query_lower = q_sub.lower()
                selected_topic = "general"
                for topic, data in self.SEARCH_TOPICS.items():
                    if topic == "general":
                        continue
                    if any(keyword in query_lower for keyword in data["keywords"]):
                        selected_topic = topic
                        break
                
                self.logger.info(f"Classified: {selected_topic.upper()}. Searching for: '{q_sub}'")
                
                suffixes = self.SEARCH_TOPICS[selected_topic]["suffixes"]
                random.shuffle(suffixes)
                
                q1 = q_sub.strip()
                q2 = f"{q1} {suffixes[0]} OR {suffixes[1]}" if len(suffixes) > 1 else q1
                q3 = f"{q1} {suffixes[2]} OR {suffixes[3]}" if len(suffixes) > 3 else q1
                
                fallback_q = f"{q_sub.strip()} {self.SEARCH_TOPICS['general']['suffixes'][0]} OR {self.SEARCH_TOPICS['general']['suffixes'][1]}"
                
                self.logger.info(f"Queries: Q1='{q1}', Q2='{q2}', Q3='{q3}'")
                
                cse0_task = asyncio.create_task(self._search_cse(q1, GOOGLE_CSE_ID, GOOGLE_CSE_API_KEY, 0, start_idx=1, force_lang="vi"))
                cse1_task = asyncio.create_task(self._search_cse(q2, GOOGLE_CSE_ID_1, GOOGLE_CSE_API_KEY_1, 1, start_idx=1, force_lang="en"))
                cse2_task = asyncio.create_task(self._search_cse(q3, GOOGLE_CSE_ID_2, GOOGLE_CSE_API_KEY_2, 2, start_idx=1, force_lang="en"))
                
                cse0_result, cse1_result, cse2_result = await asyncio.gather(
                    cse0_task, cse1_task, cse2_task, return_exceptions=True
                )
                
                def safe_result(r, name):
                    if isinstance(r, Exception):
                        self.logger.error(f"{name} error: {r}")
                        return ""
                    return r or ""
                
                cse0_result = safe_result(cse0_result, "CSE0")
                cse1_result = safe_result(cse1_result, "CSE1")
                cse2_result = safe_result(cse2_result, "CSE2")
                
                should_run_fallback = FORCE_FALLBACK_REQUEST or not (cse0_result or cse1_result or cse2_result)
                
                fallback_result = ""
                if should_run_fallback:
                    log_message = "AI requested [FORCE FALLBACK]" if FORCE_FALLBACK_REQUEST else "All CSE empty/error"
                    self.logger.warning(f"{log_message} ‚Üí Running Fallback API.")
                    
                    fallback_result = await self._run_fallback_search(fallback_q)
                    if fallback_result:
                        self.logger.info(f"Fallback success.")
                    else:
                        self.logger.warning("Fallback failed.")
                
                parts = [str(x) for x in [cse0_result, cse1_result, cse2_result, fallback_result] if x]
                
                if parts:
                    merged = "\n\n".join(parts)
                    unique_lines = []
                    seen_links = set()
                    for line in merged.splitlines():
                        match = re.search(r"\(Ngu·ªìn: (.*?)\)", line)
                        if match:
                            link = match.group(1)
                            if link not in seen_links:
                                seen_links.add(link)
                                unique_lines.append(line)
                        else:
                            unique_lines.append(line)
                    final_text = "\n".join(unique_lines)
                    final_results.append(f"### üîç [Ch·ªß ƒë·ªÅ: {selected_topic.upper()}] K·∫øt qu·∫£ cho '{q_sub}':\n{final_text.strip()}")
        
        if final_results:
            combined_final_results = "\n\n".join(final_results)
            self.set_web_search_cache(query, combined_final_results)
            self.logger.info(f"Completed search for {len(final_results)} subqueries and cached.")
            return combined_final_results
        
        self.logger.error("All 3 CSE + fallback FAILED.")
        return ""
    
    async def call_tool(self, function_call, user_id: str):
        """Dispatch tool calls to appropriate handlers."""
        name = function_call.name
        args = dict(function_call.args) if function_call.args else {}
        self.logger.info(f"TOOL CALLED: {name} | Args: {args} | User: {user_id}")
        
        try:
            if name == "web_search":
                query = args.get("query", "")
                return await self.run_search_apis(query, "general")
            
            elif name == "get_weather":
                city = args.get("city", "Ho Chi Minh City")
                data = await self.get_weather(city)
                return json.dumps(data, ensure_ascii=False, indent=2)
            
            elif name == "calculate":
                eq = args.get("equation", "")
                return await asyncio.to_thread(self.run_calculator, eq)
            
            elif name == "save_note":
                content = args.get("note_content")
                source = args.get("source", "chat_inference")
                if not content:
                    return "L·ªói: 'note_content' kh√¥ng ƒë∆∞·ª£c r·ªóng."
                return f"ƒê√£ l∆∞u note: {content}"
            
            elif name == "retrieve_notes":
                query = args.get("query", "")
                return "Kh√¥ng t√¨m th·∫•y ghi ch√∫ n√†o ph√π h·ª£p."
            
            elif name == "image_recognition":
                image_url = args.get("image_url")
                question = args.get("question")
                if not image_url or not question:
                    return "L·ªói: 'image_url' v√† 'question' kh√¥ng ƒë∆∞·ª£c r·ªóng cho image_recognition."
                return await self.run_image_recognition(image_url, question)

            else:
                return "Tool kh√¥ng t·ªìn t·∫°i!"
        
        except Exception as e:
            self.logger.error(f"Tool {name} error: {e}")
            return f"L·ªói tool: {str(e)}"
